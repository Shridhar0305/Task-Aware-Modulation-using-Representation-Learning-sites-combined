{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "injured-viewer",
   "metadata": {
    "tags": []
   },
   "source": [
    "# IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "closed-romantic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2ad51884d350>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import *\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import config2 as config\n",
    "import MODEL\n",
    "import UTILS\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-environment",
   "metadata": {
    "tags": []
   },
   "source": [
    "# HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "micro-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "    description='arguments')\n",
    "parser.add_argument('--init', type=int, default=0, help='init number')\n",
    "parser.add_argument('--fold', type=int, default=0, help='fold number')\n",
    "parser.add_argument('--few_shot_years', type=int, default=7, help='few_shot_years')\n",
    "parser.add_argument('--model_name', type=str, default='tamlstm', help='model_name')\n",
    "parser.add_argument('--date', type=str, default='20230420', help='date')\n",
    "\n",
    "args = parser.parse_args()\n",
    "#args, unknown = parser.parse_known_args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "suburban-capital",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters:test_model\n",
      "window : 30\n",
      "stride : 15\n",
      "dynamic_channels : [0, 1, 2, 3, 4]\n",
      "output_channels : [5]\n",
      "unknown : nan\n",
      "model_name : test_model\n",
      "forward_code_dim : 350\n",
      "latent_code_dim : 64\n",
      "device : cuda\n",
      "dropout : 0.4\n",
      "train : True\n",
      "batch_size : 64\n",
      "epochs : 1\n",
      "learning_rate : 0.001\n",
      "meta_learning_rate : 0.001\n",
      "num_inner_steps : 5\n",
      "init : 0\n",
      "fold : 0\n",
      "max_patience : 20\n",
      "few_shot_years : 1\n"
     ]
    }
   ],
   "source": [
    "# TIME SERIES INFO\n",
    "window = config.window\n",
    "stride = config.stride\n",
    "\n",
    "# CHANNELS INFO\n",
    "dynamic_channels = config.dynamic_channels\n",
    "output_channels = config.output_channels\n",
    "\n",
    "no_normalize_channels = config.no_normalize_channels\n",
    "normalize_channels = config.normalize_channels\n",
    "\n",
    "static_channels = config.dynamic_channels  # WE DONT USE THIS\n",
    "\n",
    "# LABELS INFO\n",
    "unknown = config.unknown\n",
    "\n",
    "# MODEL INFO\n",
    "model_name = args.model_name #\"test_model\"\n",
    "forward_code_dim = config.forward_code_dim\n",
    "latent_code_dim = config.latent_code_dim\n",
    "device = torch.device(config.device)\n",
    "dropout = config.dropout\n",
    "\n",
    "# TRAIN INFO \n",
    "train = config.train\n",
    "batch_size = config.batch_size\n",
    "epochs = config.epochs\n",
    "learning_rate = config.learning_rate\n",
    "meta_learning_rate =config.meta_learning_rate\n",
    "\n",
    "init = args.init  \n",
    "fold = 0\n",
    "num_inner_steps = 5\n",
    "max_patience = 20\n",
    "few_shot_years = 1# args.few_shot_years\n",
    "train_percent = 0.8\n",
    "\n",
    "print(\"Hyperparameters:{}\".format(model_name))\n",
    "print(\"window : {}\".format(window))\n",
    "print(\"stride : {}\".format(stride))\n",
    "print(\"dynamic_channels : {}\".format(dynamic_channels))\n",
    "print(\"output_channels : {}\".format(output_channels))\n",
    "print(\"unknown : {}\".format(unknown))\n",
    "print(\"model_name : {}\".format(model_name))\n",
    "print(\"forward_code_dim : {}\".format(forward_code_dim))\n",
    "print(\"latent_code_dim : {}\".format(latent_code_dim))\n",
    "print(\"device : {}\".format(device))\n",
    "print(\"dropout : {}\".format(dropout))\n",
    "print(\"train : {}\".format(train))\n",
    "print(\"batch_size : {}\".format(batch_size))\n",
    "print(\"epochs : {}\".format(epochs))\n",
    "print(\"learning_rate : {}\".format(learning_rate))\n",
    "print(\"meta_learning_rate : {}\".format(meta_learning_rate))\n",
    "print(\"num_inner_steps : {}\".format(num_inner_steps))\n",
    "print(\"init : {}\".format(init))\n",
    "print(\"fold : {}\".format(fold))\n",
    "print(\"max_patience : {}\".format(max_patience))\n",
    "print(\"few_shot_years : {}\".format(few_shot_years))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-tract",
   "metadata": {},
   "source": [
    "# DEFINE DIRECTORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "private-success",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATE = args.date #\"20240327\" #\n",
    "PREPROCESSED_DIR = config.PREPROCESSED_DIR\n",
    "RESULT_DIR = os.path.join(config.RESULT_DIR, DATE)\n",
    "MODEL_DIR = os.path.join(config.MODEL_DIR, DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-stanley",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "prepared-effects",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file):\n",
    "    dataset = np.load(os.path.join(PREPROCESSED_DIR, \"{}.npz\".format(file)), allow_pickle=True)\n",
    "    return dataset\n",
    "\n",
    "def get_data(dataset,preprocessed=True):\n",
    "    data = dataset[\"data\"]\n",
    "    if preprocessed:\n",
    "        data_mean = dataset[\"train_data_means\"]\n",
    "        data_std =  dataset[\"train_data_stds\"]\n",
    "\n",
    "        #print(data_mean.shape)\n",
    "        normalized_data = np.zeros_like(data)\n",
    "        if len(data.shape)==4:\n",
    "            for feature in range(data_mean.shape[0]):\n",
    "                if data_std[feature]!=0:\n",
    "                    normalized_data[:,:,:,feature] = (data[:,:,:,feature] - data_mean[feature])/data_std[feature]\n",
    "                else:\n",
    "                    normalized_data[:,:,:,feature] = data[:,:,:,feature]\n",
    "                normalized_data[:,:,:,no_normalize_channels] = data[:,:,:,no_normalize_channels]\n",
    "        else:\n",
    "            normalized_data[:,:,normalize_channels] = data[:,:,normalize_channels] - data_mean[normalize_channels]/data_std[normalize_channels]\n",
    "            # normalized_data[:,:,-1] = data[:,:,-1]\n",
    "        data = normalized_data\n",
    "    data = np.nan_to_num(data, nan=unknown)\n",
    "    #print(data.shape)\n",
    "    return data\n",
    "\n",
    "\n",
    "# def get_data(dataset, preprocessed=True):\n",
    "#     data = dataset[\"data\"]\n",
    "#     if preprocessed:\n",
    "#         data_mean = dataset[\"train_data_means\"]\n",
    "#         data_std =  dataset[\"train_data_stds\"]\n",
    "\n",
    "#         # Reshape data to (1, 145*72, 30, 24)\n",
    "#         data = np.expand_dims(data, axis=0)\n",
    "#         data = np.reshape(data, (1, -1, data.shape[2], data.shape[3]))\n",
    "\n",
    "#         # Normalize the data\n",
    "#         normalized_data = np.zeros_like(data)\n",
    "#         if len(data.shape) == 4:\n",
    "#             for feature in range(data_mean.shape[0]):\n",
    "#                 if data_std[feature] != 0:\n",
    "#                     normalized_data[:, :, :, feature] = (data[:, :, :, feature] - data_mean[feature]) / data_std[feature]\n",
    "#                 else:\n",
    "#                     normalized_data[:, :, :, feature] = data[:, :, :, feature]\n",
    "#         else:\n",
    "#             normalized_data[:, :, normalize_channels] = (data[:, :, normalize_channels] - data_mean[normalize_channels]) / data_std[normalize_channels]\n",
    "\n",
    "#         data = normalized_data\n",
    "\n",
    "#     # Replace NaN values with unknown\n",
    "#     unknown = 0  # Define unknown here, assuming it's 0\n",
    "#     data = np.nan_to_num(data, nan=unknown)\n",
    "#     return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "civil-oliver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145, 72, 30, 24)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file, index = \"strided_train\", \"in_indices\"\n",
    "dataset = load_dataset(file)\n",
    "data = get_data(dataset)\n",
    "\n",
    "#print(data[:,:,:,-1])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "genetic-definition",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3010059/1520128585.py:21: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_data[:,:,normalize_channels] = data[:,:,normalize_channels] - data_mean[normalize_channels]/data_std[normalize_channels]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15, 1095, 24)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file, index = \"test\", \"in_indices\"\n",
    "dataset = load_dataset(file)\n",
    "data = get_data(dataset)\n",
    "\n",
    "#print(data[:,:,:,-1])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "reduced-citizen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.91537482]\n",
      "[3.34140129]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train_data_stds\"][output_channels])\n",
    "print(dataset[\"train_data_means\"][output_channels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-screen",
   "metadata": {},
   "source": [
    "# TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-parade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "saved-ebony",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kumarv/vashi024/.conda/envs/main_a100/lib/python3.10/site-packages/torch/cuda/__init__.py:132: UserWarning: \n",
      "    Found GPU0 Tesla K40m which is of cuda capability 3.5.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    The minimum cuda capability supported by this library is 3.7.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, minor, min_arch // 10, min_arch % 10))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ae(\n",
      "  (instance_encoder): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (temporal_encoder): LSTM(64, 64, batch_first=True, bidirectional=True)\n",
      "  (code_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (decode_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (temporal_decoder): LSTM(64, 64, batch_first=True)\n",
      "  (instance_decoder): Linear(in_features=64, out_features=6, bias=True)\n",
      "  (static_out): Linear(in_features=64, out_features=5, bias=True)\n",
      ")\n",
      "tamlstm(\n",
      "  (encoder): TAMLSTM(\n",
      "    (_embeddings): ModuleList(\n",
      "      (0): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (1): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (2): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (3): Linear(in_features=64, out_features=700, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (out): Linear(in_features=350, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (linear): Linear(in_features=350, out_features=350, bias=True)\n",
      ")\n",
      "#Parameters:914452\n",
      "Epoch:1\tTrain Loss:nan\tValid Loss:nan\tTEST Loss:nan\tMin Loss:10000.0000\n",
      "Time:42.7239\n"
     ]
    }
   ],
   "source": [
    "if train:\n",
    "\n",
    "    # BUILD MODEL\n",
    "    inverse_model = getattr(MODEL, \"ae\")(input_channels=len(dynamic_channels)+len(output_channels), code_dim=latent_code_dim, hidden_dim=latent_code_dim, output_channels=len(static_channels), device=device)\n",
    "    inverse_model = inverse_model.to(device)\n",
    "    pytorch_total_params = sum(p.numel() for p in inverse_model.parameters() if p.requires_grad)\n",
    "    print(inverse_model)\n",
    "    forward_model = getattr(MODEL, \"tamlstm\")(input_dynamic_channels=len(dynamic_channels), input_static_channels=latent_code_dim, hidden_dim=forward_code_dim, output_channels=len(output_channels), dropout=dropout)\n",
    "    forward_model = forward_model.to(device)\n",
    "    pytorch_total_params += sum(p.numel() for p in forward_model.parameters() if p.requires_grad)\n",
    "    print(forward_model)\n",
    "    print(\"#Parameters:{}\".format(pytorch_total_params))\n",
    "    mse_criterion = torch.nn.MSELoss(reduction=\"none\")\n",
    "    optimizer = torch.optim.Adam(list(inverse_model.parameters())+list(forward_model.parameters()), lr=meta_learning_rate)\n",
    "\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    test_loss = []\n",
    "    min_loss = 10000\n",
    "    patience = 0 \n",
    "\n",
    "    for epoch in range(1,epochs+1):\n",
    "        \n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # LOSS ON TRAIN SET\n",
    "        inverse_model.train()\n",
    "        forward_model.train()\n",
    "\n",
    "        # LOAD DATA\n",
    "        file, index = \"strided_train\", \"in_indices\"\n",
    "        dataset = load_dataset(file)\n",
    "        data = get_data(dataset)\n",
    "        nodes, years, window, channels = data.shape       # HERE TAKE A LOOK\n",
    "        # GET RANDOM Train and val Basins  \n",
    "\n",
    "        # random_basin = random.sample(range(nodes), nodes)        \n",
    "        # basin_train = random_basin[:int(train_percent*nodes)]\n",
    "        # basin_valid = random_basin[int(train_percent*nodes):]     \n",
    "        data_train = data[:,:int(0.8*years),:,:]      \n",
    "        # data_train = data[basin_train,:,:,:]      \n",
    "        nodes, years, window, channels = data_train.shape    #HERE TAKE A LOOK\n",
    "\n",
    "        # GET ANCHOR AND POSITIVE YEARS\n",
    "        anchor_years = np.zeros((nodes, years))\n",
    "        for node in range(nodes):\n",
    "            anchor_years[node] = random.sample(range(years), years)\n",
    "        anchor_years = anchor_years.astype(np.int64)\n",
    "        positive_years = np.zeros((nodes, years))\n",
    "        for node in range(nodes):\n",
    "            positive_years[node] = random.sample(range(years), years)\n",
    "        positive_years = positive_years.astype(np.int64)\n",
    "        # print(anchor_years.shape, positive_years.shape)\n",
    "\n",
    "    # \t\t# GET TRAIN AND VALIDATION SET\n",
    "    # \t\tanchor_years = anchor_years[:,:int(train_percent*years)]\n",
    "    # \t\tanchor_years = anchor_years[:,int(train_percent*years):]\n",
    "    # \t\tpositive_years = positive_years[:,:int(train_percent*years)]\n",
    "    # \t\tpositive_years = positive_years[:,int(train_percent*years):]\n",
    "        # print(anchor_years.shape, anchor_years.shape, positive_years.shape, positive_years.shape)\n",
    "\n",
    "        # LOSS ON TRAIN SET\n",
    "        inverse_model.train()\n",
    "        forward_model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_recon_loss = 0\n",
    "        epoch_contrastive_loss = 0\n",
    "        epoch_static_loss = 0\n",
    "        epoch_forward_loss = 0\n",
    "        for year in range(anchor_years.shape[1]):\n",
    "\n",
    "            #Get (anchor,positive) Instances for each node\n",
    "            anchor_data = data_train[np.arange(nodes), anchor_years[:, year]]\n",
    "            positive_data = data_train[np.arange(nodes), positive_years[:, year]]\n",
    "            # print(anchor_data.shape, positive_data.shape)\n",
    "\n",
    "            # Remove pairs where (anchor,positive) years are same\n",
    "            keep_idx = anchor_years[:, year] != positive_years[:, year]\n",
    "            anchor_data = anchor_data[keep_idx]\n",
    "            positive_data = positive_data[keep_idx]\n",
    "            # print(anchor_data.shape, positive_data.shape)\n",
    "\n",
    "            # Remove pairs where (anchor,positive) basins have unknown in streamflow\n",
    "            keep_idx = np.zeros((anchor_data.shape[0], 2)).astype(bool)\n",
    "            keep_idx[:,0] = (anchor_data[:,:,-1]!=unknown).all(axis=1)\n",
    "            keep_idx[:,1] = (positive_data[:,:,-1]!=unknown).all(axis=1)\n",
    "            keep_idx = keep_idx.all(axis=1)\n",
    "            anchor_data = anchor_data[keep_idx]\n",
    "            positive_data = positive_data[keep_idx]\n",
    "            # print(anchor_data.shape, positive_data.shape)\n",
    "\n",
    "            random_batches = random.sample(range(anchor_data.shape[0]),anchor_data.shape[0])\n",
    "            for batch in range(math.ceil(anchor_data.shape[0]/batch_size)):\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # GET BATCH DATA FOR INVERSE MODEL\n",
    "                random_batch = random_batches[batch*batch_size:(batch+1)*batch_size]\n",
    "                batch_anchor_data = torch.from_numpy(anchor_data[random_batch]).to(device)\n",
    "                batch_positive_data = torch.from_numpy(positive_data[random_batch]).to(device)\n",
    "                batch_input = torch.cat((batch_anchor_data[:,:,dynamic_channels+output_channels], batch_positive_data[:,:,dynamic_channels+output_channels]), dim=0)\n",
    "                batch_static = torch.cat((batch_anchor_data[:,0,static_channels], batch_positive_data[:,0,static_channels]), axis=0)\n",
    "                # print(batch_input.shape, batch_static.shape)\n",
    "\n",
    "                # GET INVERSE OUTPUT\n",
    "                batch_code_vec, _,_,_ = inverse_model(x=batch_input.float())\n",
    "                # print(batch_code_vec.shape, batch_static_pred.shape, batch_input_pred.shape)\n",
    "\n",
    "                # GET BATCH DATA FOR FORWARD MODEL\n",
    "                batch_dynamic_input = batch_positive_data[:, :, dynamic_channels].to(device)\n",
    "                batch_static_input = batch_code_vec[:batch_anchor_data.shape[0]].unsqueeze(1).repeat(1, window, 1)\n",
    "                batch_label = batch_positive_data[:, :, output_channels].float().to(device)\n",
    "                # print(batch_dynamic_input.shape, batch_static_input.shape, batch_label.shape)\n",
    "\n",
    "                # GET FORWARD OUTPUT\n",
    "                batch_label_pred = forward_model(x_dynamic=batch_dynamic_input.float().to(device), x_static=batch_static_input.float().to(device))\n",
    "                # print(batch_label_pred.shape)\n",
    "                \n",
    "                \n",
    "                # print(\"natch pred datatype\",batch_label_pred.dtype)\n",
    "                # print(\"batch label datatype\",batch_label.dtype)\n",
    "                \n",
    "\n",
    "                # CALCULATE LOSS\n",
    "                batch_forward_loss = mse_criterion(batch_label, batch_label_pred)\t\t\t\t\t\t\t\t\t\t# FORWARD LOSS (PER CHANNEL LOSS)\n",
    "                mask = (batch_label!=unknown).float()\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# FORWARD LOSS (CREATE MASK)\n",
    "                batch_forward_loss = batch_forward_loss * mask\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# FORWARD LOSS (MULTIPLY MASK)\n",
    "                batch_forward_loss, mask = torch.sum(batch_forward_loss, dim=2), (torch.sum(mask, dim=2)>0).float()\t\t# FORWARD LOSS (PER INSTANCE LOSS)\n",
    "                batch_forward_loss = torch.sum(batch_forward_loss)/torch.sum(mask)\t\t\t\t\t\t\t\t\t\t# FORWARD LOSS (MEAN SEQUENCE LOSS)\n",
    "                batch_loss = batch_forward_loss\n",
    "                # print(batch_loss.shape, batch_loss)\n",
    "\n",
    "                # LOSS BACKPROPOGATE\n",
    "                \n",
    "#                 print(\"natch loss datatype\",batch_loss.dtype)\n",
    "                \n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # AGGREGATE LOSS\n",
    "                epoch_loss += batch_loss.item()\n",
    "                epoch_forward_loss += batch_forward_loss.item()\n",
    "\n",
    "        epoch_loss /= ((batch+1)*(year+1))\n",
    "        epoch_forward_loss /= ((batch+1)*(year+1))\n",
    "        print('Epoch:{}\\tTrain Loss:{:.4f}'.format(epoch, epoch_loss), end=\"\\t\")\n",
    "        train_loss.append(epoch_loss)\n",
    "\n",
    "        # SCORE ON VALIDATION SET\n",
    "\n",
    "        inverse_model.eval()\n",
    "        forward_model.eval()\n",
    "\n",
    "        # LOAD DATA\n",
    "        # data_valid = data[basin_valid,:,:,:]      \n",
    "        data_valid = data[:,int(0.8*years):,:,:]      \n",
    "        nodes, years, window, channels = data_valid.shape      #TAKE A LOOK HERE\n",
    "        # print(nodes, years, window, channels)\n",
    "\n",
    "        # GET ANCHOR AND POSITIVE YEARS\n",
    "        anchor_years = np.zeros((nodes, years))\n",
    "        for node in range(nodes):\n",
    "            anchor_years[node] = random.sample(range(years), years)\n",
    "        anchor_years = anchor_years.astype(np.int64)\n",
    "        positive_years = np.zeros((nodes, years))\n",
    "        for node in range(nodes):\n",
    "            positive_years[node] = random.sample(range(years), years)\n",
    "        positive_years = positive_years.astype(np.int64)\n",
    "        epoch_loss = 0\n",
    "        epoch_recon_loss = 0\n",
    "        epoch_contrastive_loss = 0\n",
    "        epoch_static_loss = 0\n",
    "        epoch_forward_loss = 0\n",
    "\n",
    "        for year in range(anchor_years.shape[1]):\n",
    "\n",
    "            #Get (anchor,positive) Instances for each node\n",
    "            anchor_data = data_valid[np.arange(nodes), anchor_years[:, year]]\n",
    "            positive_data = data_valid[np.arange(nodes), positive_years[:, year]]\n",
    "            # print(anchor_data.shape, positive_data.shape)\n",
    "\n",
    "            # Remove pairs where (anchor,positive) years are same\n",
    "            keep_idx = anchor_years[:, year] != positive_years[:, year]\n",
    "            anchor_data = anchor_data[keep_idx]\n",
    "            positive_data = positive_data[keep_idx]\n",
    "            # print(anchor_data.shape, positive_data.shape)\n",
    "\n",
    "            # Remove pairs where (anchor,positive) basins have unknown in streamflow\n",
    "            keep_idx = np.zeros((anchor_data.shape[0], 2)).astype(bool)\n",
    "            keep_idx[:,0] = (anchor_data[:,:,-1]!=unknown).all(axis=1)\n",
    "            keep_idx[:,1] = (positive_data[:,:,-1]!=unknown).all(axis=1)\n",
    "            keep_idx = keep_idx.all(axis=1)\n",
    "            anchor_data = anchor_data[keep_idx]\n",
    "            positive_data = positive_data[keep_idx]\n",
    "            # print(anchor_data.shape, positive_data.shape)\n",
    "\n",
    "            random_batches = random.sample(range(anchor_data.shape[0]),anchor_data.shape[0])\n",
    "            for batch in range(math.ceil(anchor_data.shape[0]/batch_size)):\n",
    "\n",
    "                # GET BATCH DATA FOR INVERSE MODEL\n",
    "                random_batch = random_batches[batch*batch_size:(batch+1)*batch_size]\n",
    "                batch_anchor_data = torch.from_numpy(anchor_data[random_batch]).to(device)\n",
    "                batch_positive_data = torch.from_numpy(positive_data[random_batch]).to(device)\n",
    "                batch_input = torch.cat((batch_anchor_data[:,:,dynamic_channels+output_channels], batch_positive_data[:,:,dynamic_channels+output_channels]), dim=0)\n",
    "                batch_static = torch.cat((batch_anchor_data[:,0,static_channels], batch_positive_data[:,0,static_channels]), axis=0)\n",
    "                # print(batch_input.shape, batch_static.shape)\n",
    "\n",
    "                # GET INVERSE OUTPUT\n",
    "                batch_code_vec, _,_,_ = inverse_model(x=batch_input.float())\n",
    "                # print(batch_code_vec.shape, batch_static_pred.shape, batch_input_pred.shape)\n",
    "\n",
    "                # GET BATCH DATA FOR FORWARD MODEL\n",
    "                batch_dynamic_input = batch_positive_data[:, :, dynamic_channels].to(device)\n",
    "                batch_static_input = batch_code_vec[:batch_anchor_data.shape[0]].unsqueeze(1).repeat(1, window, 1)\n",
    "                batch_label = batch_positive_data[:, :, output_channels].to(device)\n",
    "                # print(batch_dynamic_input.shape, batch_static_input.shape, batch_label.shape)\n",
    "\n",
    "                # GET FORWARD OUTPUT\n",
    "                batch_label_pred = forward_model(x_dynamic=batch_dynamic_input.float().to(device), x_static=batch_static_input.float().to(device))\n",
    "                # print(batch_label_pred.shape)\n",
    "\n",
    "                # CALCULATE LOSS\n",
    "                batch_forward_loss = mse_criterion(batch_label, batch_label_pred)\t\t\t\t\t\t\t\t\t\t# FORWARD LOSS (PER CHANNEL LOSS)\n",
    "                mask = (batch_label!=unknown).float()\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# FORWARD LOSS (CREATE MASK)\n",
    "                batch_forward_loss = batch_forward_loss * mask\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# FORWARD LOSS (MULTIPLY MASK)\n",
    "                batch_forward_loss, mask = torch.sum(batch_forward_loss, dim=2), (torch.sum(mask, dim=2)>0).float()\t\t# FORWARD LOSS (PER INSTANCE LOSS)\n",
    "                batch_forward_loss = torch.sum(batch_forward_loss)/torch.sum(mask)\t\t\t\t\t\t\t\t\t\t# FORWARD LOSS (MEAN SEQUENCE LOSS)\n",
    "                batch_loss = batch_forward_loss\n",
    "                # print(batch_loss.shape, batch_loss)\n",
    "\n",
    "                # AGGREGATE LOSS\n",
    "                epoch_loss += batch_loss.item()\n",
    "                epoch_forward_loss += batch_forward_loss.item()\n",
    "\n",
    "        epoch_loss /= ((batch+1)*(year+1))\n",
    "        epoch_forward_loss /= ((batch+1)*(year+1))\n",
    "        print('Valid Loss:{:.4f}'.format(epoch_loss), end=\"\\t\")\n",
    "        valid_loss.append(epoch_loss)\n",
    "        \n",
    "\n",
    "    # \t\tif min_loss>epoch_loss:        \n",
    "        if min_loss>epoch_forward_loss:\n",
    "            min_loss = epoch_forward_loss\n",
    "            torch.save(inverse_model.state_dict(), os.path.join(MODEL_DIR, \"{}_inverse\".format(model_name)))\n",
    "            torch.save(forward_model.state_dict(), os.path.join(MODEL_DIR, \"{}_forward\".format(model_name)))\n",
    "            \n",
    "        # SCORE ON TEST SET\n",
    "\n",
    "        inverse_model.eval()\n",
    "        forward_model.eval()\n",
    "\n",
    "        # LOAD DATA\n",
    "        file, index = \"strided_test\", \"out_indices\"\n",
    "        dataset = load_dataset(file)\n",
    "        data = get_data(dataset)\n",
    "        nodes, years, window, channels = data.shape     #TAKE A LOOK HERE \n",
    "        nodes, years, window, channels = data.shape\n",
    "        # print(nodes, years, window, channels)\n",
    "\n",
    "        # GET ANCHOR AND POSITIVE YEARS\n",
    "        anchor_years = np.zeros((nodes, years))\n",
    "        for node in range(nodes):\n",
    "            anchor_years[node] = random.sample(range(years), years)\n",
    "        anchor_years = anchor_years.astype(np.int64)\n",
    "        positive_years = np.zeros((nodes, years))\n",
    "        for node in range(nodes):\n",
    "            positive_years[node] = random.sample(range(years), years)\n",
    "        positive_years = positive_years.astype(np.int64)\n",
    "        epoch_loss = 0\n",
    "        epoch_recon_loss = 0\n",
    "        epoch_contrastive_loss = 0\n",
    "        epoch_static_loss = 0\n",
    "        epoch_forward_loss = 0\n",
    "\n",
    "        for year in range(anchor_years.shape[1]):\n",
    "\n",
    "            #Get (anchor,positive) Instances for each node\n",
    "            anchor_data = data[np.arange(nodes), anchor_years[:, year]]\n",
    "            positive_data = data[np.arange(nodes), positive_years[:, year]]\n",
    "            # print(anchor_data.shape, positive_data.shape)\n",
    "\n",
    "            # Remove pairs where (anchor,positive) years are same\n",
    "            keep_idx = anchor_years[:, year] != positive_years[:, year]\n",
    "            anchor_data = anchor_data[keep_idx]\n",
    "            positive_data = positive_data[keep_idx]\n",
    "            # print(anchor_data.shape, positive_data.shape)\n",
    "\n",
    "            # Remove pairs where (anchor,positive) basins have unknown in streamflow\n",
    "            keep_idx = np.zeros((anchor_data.shape[0], 2)).astype(bool)\n",
    "            keep_idx[:,0] = (anchor_data[:,:,-1]!=unknown).all(axis=1)\n",
    "            keep_idx[:,1] = (positive_data[:,:,-1]!=unknown).all(axis=1)\n",
    "            keep_idx = keep_idx.all(axis=1)\n",
    "            anchor_data = anchor_data[keep_idx]\n",
    "            positive_data = positive_data[keep_idx]\n",
    "            # print(anchor_data.shape, positive_data.shape)\n",
    "\n",
    "            random_batches = random.sample(range(anchor_data.shape[0]),anchor_data.shape[0])\n",
    "            for batch in range(math.ceil(anchor_data.shape[0]/batch_size)):\n",
    "\n",
    "                # GET BATCH DATA FOR INVERSE MODEL\n",
    "                random_batch = random_batches[batch*batch_size:(batch+1)*batch_size]\n",
    "                batch_anchor_data = torch.from_numpy(anchor_data[random_batch]).to(device)\n",
    "                batch_positive_data = torch.from_numpy(positive_data[random_batch]).to(device)\n",
    "                batch_input = torch.cat((batch_anchor_data[:,:,dynamic_channels+output_channels], batch_positive_data[:,:,dynamic_channels+output_channels]), dim=0)\n",
    "                batch_static = torch.cat((batch_anchor_data[:,0,static_channels], batch_positive_data[:,0,static_channels]), axis=0)\n",
    "                # print(batch_input.shape, batch_static.shape)\n",
    "\n",
    "                # GET INVERSE OUTPUT\n",
    "                batch_code_vec, _,_,_ = inverse_model(x=batch_input.float())\n",
    "                # print(batch_code_vec.shape, batch_static_pred.shape, batch_input_pred.shape)\n",
    "\n",
    "                # GET BATCH DATA FOR FORWARD MODEL\n",
    "                batch_dynamic_input = batch_positive_data[:, :, dynamic_channels].to(device)\n",
    "                batch_static_input = batch_code_vec[:batch_anchor_data.shape[0]].unsqueeze(1).repeat(1, window, 1)\n",
    "                batch_label = batch_positive_data[:, :, output_channels].float().to(device)\n",
    "                # print(batch_dynamic_input.shape, batch_static_input.shape, batch_label.shape)\n",
    "\n",
    "                # GET FORWARD OUTPUT\n",
    "                batch_label_pred = forward_model(x_dynamic=batch_dynamic_input.float().to(device), x_static=batch_static_input.float().to(device))\n",
    "                # print(batch_label_pred.shape)\n",
    "\n",
    "                # CALCULATE LOSS\n",
    "                batch_forward_loss = mse_criterion(batch_label, batch_label_pred)\t\t\t\t\t\t\t\t\t\t# FORWARD LOSS (PER CHANNEL LOSS)\n",
    "                mask = (batch_label!=unknown).float()\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# FORWARD LOSS (CREATE MASK)\n",
    "                batch_forward_loss = batch_forward_loss * mask\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# FORWARD LOSS (MULTIPLY MASK)\n",
    "                batch_forward_loss, mask = torch.sum(batch_forward_loss, dim=2), (torch.sum(mask, dim=2)>0).float()\t\t# FORWARD LOSS (PER INSTANCE LOSS)\n",
    "                batch_forward_loss = torch.sum(batch_forward_loss)/torch.sum(mask)\t\t\t\t\t\t\t\t\t\t# FORWARD LOSS (MEAN SEQUENCE LOSS)\n",
    "                batch_loss = batch_forward_loss\n",
    "                # print(batch_loss.shape, batch_loss)\n",
    "\n",
    "                # AGGREGATE LOSS\n",
    "                epoch_loss += batch_loss.item()\n",
    "                epoch_forward_loss += batch_forward_loss.item()\n",
    "\n",
    "        epoch_loss /= ((batch+1)*(year+1))\n",
    "        epoch_forward_loss /= ((batch+1)*(year+1))\n",
    "        print('TEST Loss:{:.4f}\\tMin Loss:{:.4f}'.format(epoch_loss, min_loss))\n",
    "        test_loss.append(epoch_loss)\n",
    "\n",
    "        end = time.time()\n",
    "        print(\"Time:{:.4f}\".format(end-start))            \n",
    "\n",
    "    # PLOT LOSS\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax1.set_xlabel(\"#Epoch\", fontsize=50)\n",
    "\n",
    "    # PLOT TRAIN LOSS\n",
    "    lns1 = ax1.plot(train_loss, color='red', marker='o', linewidth=4, label=\"TRAIN LOSS\")\n",
    "\n",
    "    # PLOT VALIDATION SCORE\n",
    "    ax2 = ax1.twinx()\n",
    "    lns2 = ax2.plot(valid_loss, color='blue', marker='o', linewidth=4, label=\"VAL LOSS\")\n",
    "    \n",
    "    # PLOT TEST SCORE\n",
    "    ax3 = ax1.twinx()\n",
    "    lns3 = ax3.plot(test_loss, color='green', marker='o', linewidth=4, label=\"TEST LOSS\")\n",
    "\n",
    "    # added these three lines\n",
    "    lns = lns1+lns2+lns3\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    ax1.legend(lns, labs, loc=\"upper right\", fontsize=40, frameon=False)\n",
    "\n",
    "    plt.tight_layout(pad=0.0,h_pad=0.0,w_pad=0.0)\n",
    "    plt.savefig(os.path.join(RESULT_DIR, \"{}_SCORE.pdf\".format(model_name)), format = \"pdf\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-worcester",
   "metadata": {},
   "source": [
    "# TEST MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-light",
   "metadata": {},
   "source": [
    "## Out of DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "likely-formation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape  (15, 1, 30, 24)\n",
      "ae(\n",
      "  (instance_encoder): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (temporal_encoder): LSTM(64, 64, batch_first=True, bidirectional=True)\n",
      "  (code_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (decode_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (temporal_decoder): LSTM(64, 64, batch_first=True)\n",
      "  (instance_decoder): Linear(in_features=64, out_features=6, bias=True)\n",
      "  (static_out): Linear(in_features=64, out_features=5, bias=True)\n",
      ")\n",
      "tamlstm(\n",
      "  (encoder): TAMLSTM(\n",
      "    (_embeddings): ModuleList(\n",
      "      (0): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (1): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (2): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (3): Linear(in_features=64, out_features=700, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (out): Linear(in_features=350, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (linear): Linear(in_features=350, out_features=350, bias=True)\n",
      ")\n",
      "#Parameters:914452\n",
      "dataset_code (1, 64)\n",
      "before1  (1, 64)\n",
      "after1  (24, 64)\n",
      "[3.91537482]\n",
      "1 MONTHS RMSE nan\n",
      "Time:1.2017\n",
      "train data shape  (15, 5, 30, 24)\n",
      "ae(\n",
      "  (instance_encoder): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (temporal_encoder): LSTM(64, 64, batch_first=True, bidirectional=True)\n",
      "  (code_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (decode_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (temporal_decoder): LSTM(64, 64, batch_first=True)\n",
      "  (instance_decoder): Linear(in_features=64, out_features=6, bias=True)\n",
      "  (static_out): Linear(in_features=64, out_features=5, bias=True)\n",
      ")\n",
      "tamlstm(\n",
      "  (encoder): TAMLSTM(\n",
      "    (_embeddings): ModuleList(\n",
      "      (0): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (1): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (2): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (3): Linear(in_features=64, out_features=700, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (out): Linear(in_features=350, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (linear): Linear(in_features=350, out_features=350, bias=True)\n",
      ")\n",
      "#Parameters:914452\n",
      "dataset_code (1, 64)\n",
      "before1  (1, 64)\n",
      "after1  (24, 64)\n",
      "[3.91537482]\n",
      "3 MONTHS RMSE 0.06602225330278369\n",
      "Time:1.9630\n",
      "train data shape  (15, 11, 30, 24)\n",
      "ae(\n",
      "  (instance_encoder): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (temporal_encoder): LSTM(64, 64, batch_first=True, bidirectional=True)\n",
      "  (code_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (decode_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (temporal_decoder): LSTM(64, 64, batch_first=True)\n",
      "  (instance_decoder): Linear(in_features=64, out_features=6, bias=True)\n",
      "  (static_out): Linear(in_features=64, out_features=5, bias=True)\n",
      ")\n",
      "tamlstm(\n",
      "  (encoder): TAMLSTM(\n",
      "    (_embeddings): ModuleList(\n",
      "      (0): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (1): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (2): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (3): Linear(in_features=64, out_features=700, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (out): Linear(in_features=350, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (linear): Linear(in_features=350, out_features=350, bias=True)\n",
      ")\n",
      "#Parameters:914452\n",
      "dataset_code (1, 64)\n",
      "before1  (1, 64)\n",
      "after1  (24, 64)\n",
      "[3.91537482]\n",
      "6 MONTHS RMSE nan\n",
      "Time:2.9859\n",
      "train data shape  (15, 23, 30, 24)\n",
      "ae(\n",
      "  (instance_encoder): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (temporal_encoder): LSTM(64, 64, batch_first=True, bidirectional=True)\n",
      "  (code_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (decode_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (temporal_decoder): LSTM(64, 64, batch_first=True)\n",
      "  (instance_decoder): Linear(in_features=64, out_features=6, bias=True)\n",
      "  (static_out): Linear(in_features=64, out_features=5, bias=True)\n",
      ")\n",
      "tamlstm(\n",
      "  (encoder): TAMLSTM(\n",
      "    (_embeddings): ModuleList(\n",
      "      (0): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (1): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (2): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (3): Linear(in_features=64, out_features=700, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (out): Linear(in_features=350, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (linear): Linear(in_features=350, out_features=350, bias=True)\n",
      ")\n",
      "#Parameters:914452\n",
      "dataset_code (1, 64)\n",
      "before1  (1, 64)\n",
      "after1  (24, 64)\n",
      "[3.91537482]\n",
      "12 MONTHS RMSE nan\n",
      "Time:5.1216\n",
      "train data shape  (15, 47, 30, 24)\n",
      "ae(\n",
      "  (instance_encoder): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (temporal_encoder): LSTM(64, 64, batch_first=True, bidirectional=True)\n",
      "  (code_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (decode_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (temporal_decoder): LSTM(64, 64, batch_first=True)\n",
      "  (instance_decoder): Linear(in_features=64, out_features=6, bias=True)\n",
      "  (static_out): Linear(in_features=64, out_features=5, bias=True)\n",
      ")\n",
      "tamlstm(\n",
      "  (encoder): TAMLSTM(\n",
      "    (_embeddings): ModuleList(\n",
      "      (0): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (1): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (2): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (3): Linear(in_features=64, out_features=700, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (out): Linear(in_features=350, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (linear): Linear(in_features=350, out_features=350, bias=True)\n",
      ")\n",
      "#Parameters:914452\n",
      "dataset_code (1, 64)\n",
      "before1  (1, 64)\n",
      "after1  (24, 64)\n",
      "[3.91537482]\n",
      "24 MONTHS RMSE nan\n",
      "Time:9.4526\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "all_few_shot_years = [1,3,6,12,24]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_inner_steps = 0\n",
    "\n",
    "for few_shot_years in all_few_shot_years:\n",
    "    start = time.time()\n",
    "\n",
    "\n",
    "    # num_years = 9\n",
    "\n",
    "\n",
    "\n",
    "    #     file, index = \"strided_test\", \"in_indices\"\n",
    "    #     dataset = load_dataset(file)\n",
    "    #     data_test = get_data(dataset, index,fold=fold)\n",
    "    #     nodes, years, window, channels = data_test.shape\n",
    "    file, index = \"strided_test\" ,\"out_indices\"\n",
    "    dataset = load_dataset(file)\n",
    "    data = get_data(dataset)  #[15,72,30,6]\n",
    "#     data = data.reshape((1, years * nodes, window, len(output_channels)))\n",
    "    data_test = data[:,48:]\n",
    "#     data = data.reshape((1, years * nodes, window, len(output_channels)))\n",
    "\n",
    "    nodes, years, window, channels = data_test.shape     #TAKE A LOOK HERE\n",
    "    dataset_true = unknown*np.ones((nodes, years, window, len(output_channels)), dtype=np.float32)\n",
    "#     dataset_true = unknown*np.ones((1, years * nodes, window, len(output_channels)), dtype=np.float32)\n",
    "    dataset_pred = unknown*np.ones((nodes, years, window, len(output_channels)), dtype=np.float32)\n",
    "#     dataset_pred = unknown*np.ones((1, years * nodes, window, len(output_channels)), dtype=np.float32)\n",
    "\n",
    "    train_data = data[:,-((few_shot_years*2)-1):]  #HERE\n",
    "    print(\"train data shape \", train_data.shape)\n",
    "    nodes, years, window, channels = train_data.shape  \n",
    "    train_data = train_data.reshape((1, years * nodes, window, -1))\n",
    "    \n",
    "    nodes, years, window, channels = train_data.shape    #TAKE A LOOK HERE\n",
    "#     print(train_data.shape)\n",
    "\n",
    "    # train_data = train_data[:,:(num_years*2-1),:,:]\n",
    "    # train_data = train_data[:,years-num_years*2+1:,:,:]\n",
    "    nodes, years, window, channels = train_data.shape     #TAKE A LOOK HERE\n",
    "#     print(train_data.shape)\n",
    "    \n",
    "    file, index = \"strided_test\", \"out_indices\"\n",
    "\n",
    "\n",
    "    # BUILD MODEL\n",
    "    inverse_model = getattr(MODEL, \"ae\")(input_channels=len(dynamic_channels)+len(output_channels), code_dim=latent_code_dim, hidden_dim=latent_code_dim, output_channels=len(static_channels), device=device)\n",
    "    inverse_model = inverse_model.to(device)\n",
    "    pytorch_total_params = sum(p.numel() for p in inverse_model.parameters() if p.requires_grad)\n",
    "    print(inverse_model)\n",
    "    forward_model = getattr(MODEL, \"tamlstm\")(input_dynamic_channels=len(dynamic_channels), input_static_channels=latent_code_dim, hidden_dim=forward_code_dim, output_channels=len(output_channels), dropout=dropout)\n",
    "    forward_model = forward_model.to(device)\n",
    "    pytorch_total_params += sum(p.numel() for p in forward_model.parameters() if p.requires_grad)\n",
    "    print(forward_model)\n",
    "    print(\"#Parameters:{}\".format(pytorch_total_params))\n",
    "    criterion = torch.nn.MSELoss(reduction=\"none\")\n",
    "    test_criterion = torch.nn.MSELoss(reduction=\"mean\")\n",
    "    optimizer_embedding = torch.optim.Adam(list(inverse_model.parameters()), lr=learning_rate)\n",
    "    optimizer_forward = torch.optim.Adam(list(forward_model.parameters()), lr=learning_rate)\n",
    "    # print(\"#Parameters:{}\".format(pytorch_total_params))\n",
    "    # print(model)\n",
    "\n",
    "    # LOAD MODEL\n",
    "    inverse_model.load_state_dict(torch.load(os.path.join(MODEL_DIR, \"{}_inverse\".format(model_name))))\n",
    "    inverse_model.eval()\n",
    "    forward_model.load_state_dict(torch.load(os.path.join(MODEL_DIR, \"{}_forward\".format(model_name))))\n",
    "    dataset_code = unknown*np.ones((nodes, years, latent_code_dim), dtype=np.float32)\n",
    "\n",
    "\n",
    "    for year in range(years):\n",
    "\n",
    "        #Get instances for each node\n",
    "        node_data = train_data[np.arange(nodes), year]\n",
    "        # print(node_data.shape)\n",
    "\n",
    "        # Remove pairs where node have unknown in streamflow\n",
    "        keep_idx = np.zeros((node_data.shape[0], 1)).astype(bool)\n",
    "        keep_idx[:,0] = (node_data[:,:,-1]!=unknown).all(axis=1)\n",
    "        keep_idx = keep_idx.all(axis=1)\n",
    "        node_data = node_data[keep_idx]\n",
    "        # print(node_data.shape)\n",
    "\n",
    "        for batch in range(math.ceil(node_data.shape[0]/batch_size)):\n",
    "\n",
    "            # GET BATCH DATA FOR INVERSE MODEL\n",
    "            batch_data = torch.from_numpy(node_data[batch*batch_size:(batch+1)*batch_size]).to(device)\n",
    "            batch_input = batch_data[:,:,dynamic_channels+output_channels]\n",
    "            batch_static = batch_data[:,0,static_channels]\n",
    "            # print(batch_input.shape, batch_static.shape)\n",
    "\n",
    "            # GET INVERSE OUTPUT\n",
    "            batch_code_vec, batch_static_pred, batch_input_pred,_ = inverse_model(x=batch_input.float())\n",
    "            # print(batch_code_vec.shape, batch_static_pred.shape, batch_input_pred.shape)\n",
    "\n",
    "            # STORE OUTPUT\n",
    "            dataset_code[batch*batch_size:(batch+1)*batch_size, year] = batch_code_vec.detach().cpu().numpy()\n",
    "    dataset_code = np.mean(dataset_code, axis=1)\n",
    "    print(\"dataset_code\",dataset_code.shape)\n",
    "    total_loss = 0\n",
    "    #for basin in range():\n",
    "    for basin in range(nodes):\n",
    "        # print(basin)\n",
    "        forward_model.train()\n",
    "\n",
    "        # Adapt\n",
    "        for epoch in range(num_inner_steps):\n",
    "            # print(\"I AM HERE epoch\",epoch)\n",
    "            # LOSS ON TRAIN SET\n",
    "            forward_model.load_state_dict(torch.load(os.path.join(MODEL_DIR, \"{}_forward\".format(model_name))))\n",
    "            # LOAD DATA\n",
    "            # print(nodes, years, window, channels)\n",
    "\n",
    "            for year in range(years):\n",
    "\n",
    "                #Get instance for each node\n",
    "    #             print(\"node DATA\",node_data.shape)\n",
    "                node_data = train_data[basin, year]\n",
    "                # print(node_data.shape)\n",
    "\n",
    "\n",
    "                optimizer_forward.zero_grad()\n",
    "\n",
    "                # GET BATCH DATA AND LABEL\n",
    "    #             print(\"node_data\",node_data.shape)\n",
    "                batch_data = torch.from_numpy(node_data).unsqueeze(0).to(device)\n",
    "                batch_dynamic_input = batch_data[:, :, dynamic_channels].to(device)\n",
    "                batch_static_input = batch_data[:, :, static_channels].to(device)\n",
    "                batch_label = batch_data[:, :, output_channels].float().to(device)\n",
    "                # print(batch_input.shape, batch_label.shape)\n",
    "\n",
    "                # GET INVERSE OUTPUT\n",
    "    #             print(\"batch_code_vec\",dataset_code[basin].shape)\n",
    "                batch_static_input = torch.from_numpy(dataset_code[basin]).to(device).unsqueeze(0).unsqueeze(1).repeat(1, window, 1)\n",
    "\n",
    "                # GET FORWARD OUTPUT\n",
    "    #             print(\"batch_code_vec_repeat\",batch_static_input.shape)\n",
    "    #             print(\"batch_dynamic_input\",batch_dynamic_input.shape)\n",
    "                batch_label_pred = forward_model(x_dynamic=batch_dynamic_input.float().to(device), x_static=batch_static_input.float().to(device))\n",
    "                # print(batch_pred.shape)\n",
    "\n",
    "                # CALCULATE LOSS\n",
    "                batch_loss = criterion(batch_label, batch_label_pred)\t\t\t\t\t\t\t\t\t\t\t# PER CHANNEL LOSS\n",
    "                mask = (batch_label!=unknown).float()\t\t\t\t\t\t\t\t\t\t\t\t\t# CREATE MASK\n",
    "                batch_loss = batch_loss * mask\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# MULTIPLY MASK\n",
    "                batch_loss, mask = torch.sum(batch_loss, dim=2), (torch.sum(mask, dim=2)>0).float()\t\t# PER INSTANCE LOSS\n",
    "                batch_loss = torch.sum(batch_loss)/torch.sum(mask)\t\t\t\t\t\t\t\t\t\t# MEAN SEQUENCE LOSS\n",
    "                # print(batch_loss.shape)\n",
    "\n",
    "                # LOSS BACKPROPOGATE\n",
    "                batch_loss.backward()\n",
    "                optimizer_forward.step()\n",
    "    \n",
    "        \"TESTING\"\n",
    "        #YOUR PRED SHOULD BE COMBINED BUT THE LABEL SHOULD BE BROKEN\n",
    "        \n",
    "    print(\"before1 \", dataset_code.shape)\n",
    "    dataset_code_repeat = np.repeat(dataset_code, data_test.shape[0], axis=0)\n",
    "    print(\"after1 \", dataset_code_repeat.shape)\n",
    "    for basin in range(data_test.shape[0]):\n",
    "        inverse_model.eval()\n",
    "        forward_model.eval()\n",
    "    #     file, index = \"strided_test\", \"test_index\"\n",
    "    #     dataset = load_dataset(file)\n",
    "    #     data_test = get_data(dataset, index)\n",
    "        _,years_test,_,_ = data_test.shape\n",
    "        # for year in range(years_test):\n",
    "\n",
    "        #Get instance for each node\n",
    "        node_data_test = data_test[basin, :]\n",
    "        #print(node_data_test.shape)\n",
    "\n",
    "        # GET BATCH DATA AND LABEL\n",
    "        batch_data = torch.from_numpy(node_data_test).unsqueeze(0).to(device)\n",
    "        batch_dynamic_input = batch_data[:, :,:,dynamic_channels].to(device)\n",
    "        batch_static_input = batch_data[:, :,:,static_channels].to(device)\n",
    "        batch_label = batch_data[:, :,:,output_channels].to(device)\n",
    "        # print(batch_input.shape, batch_label.shape)\n",
    "\n",
    "        # GET INVERSE OUTPUT\n",
    "#         print(\"batch_code_vec\",dataset_code[basin].shape\n",
    "        batch_static_input = torch.from_numpy(dataset_code_repeat[basin]).to(device).unsqueeze(0).unsqueeze(1).repeat(1,1, window, 1)\n",
    "        batch_static_input = batch_static_input.repeat(1,years_test, 1, 1).squeeze()\n",
    "\n",
    "        # GET OUTPUT\n",
    "        \n",
    "        # print(batch_dynamic_input.squeeze().shape)\n",
    "        # print(batch_static_input.shape)\n",
    "        \n",
    "        batch_pred = forward_model(x_dynamic=batch_dynamic_input.squeeze().float(), x_static=batch_static_input.float())\n",
    "        \n",
    "        # print(batch_pred.shape)\n",
    "        # print(batch_label.shape)\n",
    "\n",
    "        # print(test_criterion(batch_label,batch_pred).item())\n",
    "        total_loss+= test_criterion(batch_label,batch_pred).item()\n",
    "\n",
    "\n",
    "        # STORE OUTPUT\n",
    "#             print(dataset_true.shape)\n",
    "#         print(\"size of batchlabel\", batch_label.shape)\n",
    "#         print(\"size of batchpred\", batch_pred.shape)\n",
    "#         print(\"size of dataset true\", dataset_true.shape)\n",
    "#         print(\"size of dataset pred\", dataset_pred.shape)\n",
    "        \n",
    "        dataset_true[basin, :,:,:] = batch_label.detach().cpu().numpy()\n",
    "        dataset_pred[basin, :,:,:] = batch_pred.detach().cpu().numpy()\n",
    "\n",
    "    # dataset_true = (dataset_true*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels]\n",
    "    # dataset_pred = (dataset_pred*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels]\n",
    "#     print(\"std\")\n",
    "#     print(dataset[\"train_data_stds\"])\n",
    "    print(dataset[\"train_data_stds\"][output_channels])\n",
    "    \n",
    "    total_loss/=(nodes)\n",
    "    print(few_shot_years, \"MONTHS\" , \"RMSE\", np.sqrt(total_loss))\n",
    "    \n",
    "    np.save(os.path.join(RESULT_DIR, \"{}_{}_{}\".format(file, index, \"true_{}\".format(fold))), dataset_true)\n",
    "    np.save(os.path.join(RESULT_DIR, \"{}_{}_{}_{}\".format(file, index, few_shot_years, model_name)), dataset_pred)\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Time:{:.4f}\".format(end-start))\n",
    "#         inverse_model.eval()\n",
    "#         forward_model.eval()\n",
    "#         _,years_test,_,_ = data_test.shape\n",
    "#         # for year in range(years_test):\n",
    "\n",
    "#         #Get instance for each node\n",
    "#         node_data_test = data_test[basin, :]\n",
    "# #         print(\"before \", dataset_code.shape)\n",
    "# # #         dataset_code = np.repeat(dataset_code, node_data_test.shape, axis=0)\n",
    "# #         print(\"after \", dataset_code.shape)\n",
    "# #         print(node_data_test.shape)\n",
    "\n",
    "#         # GET BATCH DATA AND LABEL\n",
    "#         batch_data = torch.from_numpy(node_data_test).unsqueeze(0).to(device)\n",
    "#         batch_label = batch_data[:, :,:,output_channels].to(device)\n",
    "        \n",
    "        \n",
    "#         batch_data = torch.from_numpy(node_data_test).unsqueeze(0).to(device)\n",
    "#         batch_dynamic_input = batch_data[:, :,:,dynamic_channels].to(device)\n",
    "# #         batch_static_input = batch_data[:, :,:,static_channels].to(device)\n",
    "# #         batch_label = batch_data[:, :,:,output_channels].to(device)\n",
    "#         # print(batch_input.shape, batch_label.shape)\n",
    "\n",
    "#         # GET INVERSE OUTPUT\n",
    "# #         print(\"batch_code_vec\",dataset_code[basin].shape\n",
    "#         batch_static_input = torch.from_numpy(dataset_code_repeat[basin]).to(device).unsqueeze(0).unsqueeze(1).repeat(1,1, window, 1)\n",
    "#         batch_static_input = batch_static_input.repeat(1,years_test, 1, 1).squeeze()\n",
    "\n",
    "#         # GET OUTPUT\n",
    "#         batch_pred = forward_model(x_dynamic=batch_dynamic_input.squeeze().float(), x_static=batch_static_input.float())\n",
    "#         data_test_original = data[:,48:]\n",
    "#         data_shape = data_test_original.shape\n",
    "#         original_years = data_shape[1]\n",
    "#         original_channels = data_shape[2]\n",
    "        \n",
    "#         #HERE I AM CHANGING THE DATA TEST\n",
    "#         data_test = np.reshape(data_test_original, (1, data_shape[0]*data_shape[1], data_shape[2], data_shape[3]))\n",
    "        \n",
    "#         _,years_test,_,_ = data_test.shape\n",
    "# #         print(data_test.shape)\n",
    "#         node_data_test = data_test[0, :]\n",
    "        \n",
    "#         batch_data = torch.from_numpy(node_data_test).unsqueeze(0).to(device)\n",
    "#         batch_dynamic_input = batch_data[:, :,:,dynamic_channels].to(device)\n",
    "# #         batch_static_input = batch_data[:, :,:,static_channels].to(device)\n",
    "#         batch_label = batch_data[:, :,:,output_channels].to(device)\n",
    "#         # print(batch_input.shape, batch_label.shape)\n",
    "\n",
    "#         # GET INVERSE OUTPUT\n",
    "# #         print(\"batch_code_vec\",dataset_code[basin].shape\n",
    "#         batch_static_input = torch.from_numpy(dataset_code_repeat[basin]).to(device).unsqueeze(0).unsqueeze(1).repeat(1,1, window, 1)\n",
    "#         batch_static_input = batch_static_input.repeat(1,years_test, 1, 1).squeeze()\n",
    "\n",
    "#         # GET OUTPUT\n",
    "        \n",
    "#         batch_pred = forward_model(x_dynamic=batch_dynamic_input.squeeze().float(), x_static=batch_static_input.float())\n",
    "# #         batch_pred_shape = batch_pred.shape\n",
    "# #         print(batch_pred.shape)\n",
    "# #         batch_pred = batch_pred.reshape((original_years, original_channels, 1))\n",
    "        \n",
    "#         # print(test_criterion(batch_label,batch_pred).item())\n",
    "#         total_loss+= test_criterion(batch_label,batch_pred).item()\n",
    "\n",
    "\n",
    "#         # STORE OUTPUT\n",
    "#         dataset_true[0, :,:,:] = batch_label.detach().cpu().numpy()\n",
    "#         dataset_pred[0, :,:,:] = batch_pred.detach().cpu().numpy()\n",
    "\n",
    "#     # dataset_true = (dataset_true*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels]\n",
    "#     # dataset_pred = (dataset_pred*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels]\n",
    "# #     print(dataset[\"train_data_stds\"])\n",
    "#     print(dataset[\"train_data_stds\"][output_channels])\n",
    "    \n",
    "#     total_loss/=(nodes)\n",
    "#     print(few_shot_years, \"MONTHS\" , \"RMSE\", np.sqrt(total_loss))\n",
    "    \n",
    "#     np.save(os.path.join(RESULT_DIR, \"{}_{}_{}\".format(file, index, \"true_{}\".format(fold))), dataset_true)\n",
    "#     np.save(os.path.join(RESULT_DIR, \"{}_{}_{}_{}\".format(file, index, few_shot_years, model_name)), dataset_pred)\n",
    "\n",
    "\n",
    "#     end = time.time()\n",
    "#     print(\"Time:{:.4f}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-portsmouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = criterion(torch.tensor(dataset_true),torch.tensor(dataset_pred))\n",
    "\n",
    "print(\"pre adapt\")\n",
    "\n",
    "print(test_loss.shape)\n",
    "print(torch.sqrt((torch.mean(test_loss, dim=(1,2,3)))))\n",
    "\n",
    "\n",
    "print(\"mean\",torch.mean(torch.sqrt((torch.mean(test_loss, dim=(1,2,3))))))\n",
    "print(\"median\",torch.median(torch.sqrt((torch.mean(test_loss, dim=(1,2,3))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "all_few_shot_years = [1,3,6,12,24]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_inner_steps = 5\n",
    "\n",
    "for few_shot_years in all_few_shot_years:\n",
    "    start = time.time()\n",
    "\n",
    "\n",
    "    # num_years = 9\n",
    "\n",
    "\n",
    "\n",
    "    #     file, index = \"strided_test\", \"in_indices\"\n",
    "    #     dataset = load_dataset(file)\n",
    "    #     data_test = get_data(dataset, index,fold=fold)\n",
    "    #     nodes, years, window, channels = data_test.shape\n",
    "    file, index = \"strided_test\" ,\"out_indices\"\n",
    "    dataset = load_dataset(file)\n",
    "    data = get_data(dataset)  #[15,72,30,6]\n",
    "#     data = data.reshape((1, years * nodes, window, len(output_channels)))\n",
    "    data_test = data[:,48:]\n",
    "#     data = data.reshape((1, years * nodes, window, len(output_channels)))\n",
    "\n",
    "    nodes, years, window, channels = data_test.shape     #TAKE A LOOK HERE\n",
    "    dataset_true = unknown*np.ones((nodes, years, window, len(output_channels)), dtype=np.float32)\n",
    "#     dataset_true = unknown*np.ones((1, years * nodes, window, len(output_channels)), dtype=np.float32)\n",
    "    dataset_pred = unknown*np.ones((nodes, years, window, len(output_channels)), dtype=np.float32)\n",
    "#     dataset_pred = unknown*np.ones((1, years * nodes, window, len(output_channels)), dtype=np.float32)\n",
    "\n",
    "    train_data = data[:,-((few_shot_years*2)-1):]  #HERE\n",
    "    print(\"train data shape \", train_data.shape)\n",
    "    nodes, years, window, channels = train_data.shape  \n",
    "    train_data = train_data.reshape((1, years * nodes, window, -1))\n",
    "    \n",
    "    nodes, years, window, channels = train_data.shape    #TAKE A LOOK HERE\n",
    "#     print(train_data.shape)\n",
    "\n",
    "    # train_data = train_data[:,:(num_years*2-1),:,:]\n",
    "    # train_data = train_data[:,years-num_years*2+1:,:,:]\n",
    "    nodes, years, window, channels = train_data.shape     #TAKE A LOOK HERE\n",
    "#     print(train_data.shape)\n",
    "    \n",
    "    file, index = \"strided_test\", \"out_indices\"\n",
    "\n",
    "\n",
    "    # BUILD MODEL\n",
    "    inverse_model = getattr(MODEL, \"ae\")(input_channels=len(dynamic_channels)+len(output_channels), code_dim=latent_code_dim, hidden_dim=latent_code_dim, output_channels=len(static_channels), device=device)\n",
    "    inverse_model = inverse_model.to(device)\n",
    "    pytorch_total_params = sum(p.numel() for p in inverse_model.parameters() if p.requires_grad)\n",
    "    print(inverse_model)\n",
    "    forward_model = getattr(MODEL, \"tamlstm\")(input_dynamic_channels=len(dynamic_channels), input_static_channels=latent_code_dim, hidden_dim=forward_code_dim, output_channels=len(output_channels), dropout=dropout)\n",
    "    forward_model = forward_model.to(device)\n",
    "    pytorch_total_params += sum(p.numel() for p in forward_model.parameters() if p.requires_grad)\n",
    "    print(forward_model)\n",
    "    print(\"#Parameters:{}\".format(pytorch_total_params))\n",
    "    criterion = torch.nn.MSELoss(reduction=\"none\")\n",
    "    test_criterion = torch.nn.MSELoss(reduction=\"mean\")\n",
    "    optimizer_embedding = torch.optim.Adam(list(inverse_model.parameters()), lr=learning_rate)\n",
    "    optimizer_forward = torch.optim.Adam(list(forward_model.parameters()), lr=learning_rate)\n",
    "    # print(\"#Parameters:{}\".format(pytorch_total_params))\n",
    "    # print(model)\n",
    "\n",
    "    # LOAD MODEL\n",
    "    inverse_model.load_state_dict(torch.load(os.path.join(MODEL_DIR, \"{}_inverse\".format(model_name))))\n",
    "    inverse_model.eval()\n",
    "    forward_model.load_state_dict(torch.load(os.path.join(MODEL_DIR, \"{}_forward\".format(model_name))))\n",
    "    dataset_code = unknown*np.ones((nodes, years, latent_code_dim), dtype=np.float32)\n",
    "\n",
    "\n",
    "    for year in range(years):\n",
    "\n",
    "        #Get instances for each node\n",
    "        node_data = train_data[np.arange(nodes), year]\n",
    "        # print(node_data.shape)\n",
    "\n",
    "        # Remove pairs where node have unknown in streamflow\n",
    "        keep_idx = np.zeros((node_data.shape[0], 1)).astype(bool)\n",
    "        keep_idx[:,0] = (node_data[:,:,-1]!=unknown).all(axis=1)\n",
    "        keep_idx = keep_idx.all(axis=1)\n",
    "        node_data = node_data[keep_idx]\n",
    "        # print(node_data.shape)\n",
    "\n",
    "        for batch in range(math.ceil(node_data.shape[0]/batch_size)):\n",
    "\n",
    "            # GET BATCH DATA FOR INVERSE MODEL\n",
    "            batch_data = torch.from_numpy(node_data[batch*batch_size:(batch+1)*batch_size]).to(device)\n",
    "            batch_input = batch_data[:,:,dynamic_channels+output_channels]\n",
    "            batch_static = batch_data[:,0,static_channels]\n",
    "            # print(batch_input.shape, batch_static.shape)\n",
    "\n",
    "            # GET INVERSE OUTPUT\n",
    "            batch_code_vec, batch_static_pred, batch_input_pred,_ = inverse_model(x=batch_input.float())\n",
    "            # print(batch_code_vec.shape, batch_static_pred.shape, batch_input_pred.shape)\n",
    "\n",
    "            # STORE OUTPUT\n",
    "            dataset_code[batch*batch_size:(batch+1)*batch_size, year] = batch_code_vec.detach().cpu().numpy()\n",
    "    dataset_code = np.mean(dataset_code, axis=1)\n",
    "    print(\"dataset_code\",dataset_code.shape)\n",
    "    total_loss = 0\n",
    "    #for basin in range():\n",
    "    for basin in range(nodes):\n",
    "        # print(basin)\n",
    "        forward_model.train()\n",
    "\n",
    "        # Adapt\n",
    "        for epoch in range(num_inner_steps):\n",
    "            # print(\"I AM HERE epoch\",epoch)\n",
    "            # LOSS ON TRAIN SET\n",
    "            forward_model.load_state_dict(torch.load(os.path.join(MODEL_DIR, \"{}_forward\".format(model_name))))\n",
    "            # LOAD DATA\n",
    "            # print(nodes, years, window, channels)\n",
    "\n",
    "            for year in range(years):\n",
    "\n",
    "                #Get instance for each node\n",
    "    #             print(\"node DATA\",node_data.shape)\n",
    "                node_data = train_data[basin, year]\n",
    "                # print(node_data.shape)\n",
    "\n",
    "\n",
    "                optimizer_forward.zero_grad()\n",
    "\n",
    "                # GET BATCH DATA AND LABEL\n",
    "    #             print(\"node_data\",node_data.shape)\n",
    "                batch_data = torch.from_numpy(node_data).unsqueeze(0).to(device)\n",
    "                batch_dynamic_input = batch_data[:, :, dynamic_channels].to(device)\n",
    "                batch_static_input = batch_data[:, :, static_channels].to(device)\n",
    "                batch_label = batch_data[:, :, output_channels].float().to(device)\n",
    "                # print(batch_input.shape, batch_label.shape)\n",
    "\n",
    "                # GET INVERSE OUTPUT\n",
    "    #             print(\"batch_code_vec\",dataset_code[basin].shape)\n",
    "                batch_static_input = torch.from_numpy(dataset_code[basin]).to(device).unsqueeze(0).unsqueeze(1).repeat(1, window, 1)\n",
    "\n",
    "                # GET FORWARD OUTPUT\n",
    "    #             print(\"batch_code_vec_repeat\",batch_static_input.shape)\n",
    "    #             print(\"batch_dynamic_input\",batch_dynamic_input.shape)\n",
    "                batch_label_pred = forward_model(x_dynamic=batch_dynamic_input.float().to(device), x_static=batch_static_input.float().to(device))\n",
    "                # print(batch_pred.shape)\n",
    "\n",
    "                # CALCULATE LOSS\n",
    "                batch_loss = criterion(batch_label, batch_label_pred)\t\t\t\t\t\t\t\t\t\t\t# PER CHANNEL LOSS\n",
    "                mask = (batch_label!=unknown).float()\t\t\t\t\t\t\t\t\t\t\t\t\t# CREATE MASK\n",
    "                batch_loss = batch_loss * mask\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# MULTIPLY MASK\n",
    "                batch_loss, mask = torch.sum(batch_loss, dim=2), (torch.sum(mask, dim=2)>0).float()\t\t# PER INSTANCE LOSS\n",
    "                batch_loss = torch.sum(batch_loss)/torch.sum(mask)\t\t\t\t\t\t\t\t\t\t# MEAN SEQUENCE LOSS\n",
    "                # print(batch_loss.shape)\n",
    "\n",
    "                # LOSS BACKPROPOGATE\n",
    "                batch_loss.backward()\n",
    "                optimizer_forward.step()\n",
    "    \n",
    "        \"TESTING\"\n",
    "        #YOUR PRED SHOULD BE COMBINED BUT THE LABEL SHOULD BE BROKEN\n",
    "        \n",
    "    print(\"before1 \", dataset_code.shape)\n",
    "    dataset_code_repeat = np.repeat(dataset_code, data_test.shape[0], axis=0)\n",
    "    print(\"after1 \", dataset_code_repeat.shape)\n",
    "    for basin in range(data_test.shape[0]):\n",
    "        inverse_model.eval()\n",
    "        forward_model.eval()\n",
    "    #     file, index = \"strided_test\", \"test_index\"\n",
    "    #     dataset = load_dataset(file)\n",
    "    #     data_test = get_data(dataset, index)\n",
    "        _,years_test,_,_ = data_test.shape\n",
    "        # for year in range(years_test):\n",
    "\n",
    "        #Get instance for each node\n",
    "        node_data_test = data_test[basin, :]\n",
    "        #print(node_data_test.shape)\n",
    "\n",
    "        # GET BATCH DATA AND LABEL\n",
    "        batch_data = torch.from_numpy(node_data_test).unsqueeze(0).to(device)\n",
    "        batch_dynamic_input = batch_data[:, :,:,dynamic_channels].to(device)\n",
    "        batch_static_input = batch_data[:, :,:,static_channels].to(device)\n",
    "        batch_label = batch_data[:, :,:,output_channels].to(device)\n",
    "        # print(batch_input.shape, batch_label.shape)\n",
    "\n",
    "        # GET INVERSE OUTPUT\n",
    "#         print(\"batch_code_vec\",dataset_code[basin].shape\n",
    "        batch_static_input = torch.from_numpy(dataset_code_repeat[basin]).to(device).unsqueeze(0).unsqueeze(1).repeat(1,1, window, 1)\n",
    "        batch_static_input = batch_static_input.repeat(1,years_test, 1, 1).squeeze()\n",
    "\n",
    "        # GET OUTPUT\n",
    "        \n",
    "        # print(batch_dynamic_input.squeeze().shape)\n",
    "        # print(batch_static_input.shape)\n",
    "        \n",
    "        batch_pred = forward_model(x_dynamic=batch_dynamic_input.squeeze().float(), x_static=batch_static_input.float())\n",
    "        \n",
    "        # print(batch_pred.shape)\n",
    "        # print(batch_label.shape)\n",
    "\n",
    "        # print(test_criterion(batch_label,batch_pred).item())\n",
    "        total_loss+= test_criterion(batch_label,batch_pred).item()\n",
    "\n",
    "\n",
    "        # STORE OUTPUT\n",
    "#             print(dataset_true.shape)\n",
    "#         print(\"size of batchlabel\", batch_label.shape)\n",
    "#         print(\"size of batchpred\", batch_pred.shape)\n",
    "#         print(\"size of dataset true\", dataset_true.shape)\n",
    "#         print(\"size of dataset pred\", dataset_pred.shape)\n",
    "        \n",
    "        dataset_true[basin, :,:,:] = batch_label.detach().cpu().numpy()\n",
    "        dataset_pred[basin, :,:,:] = batch_pred.detach().cpu().numpy()\n",
    "\n",
    "    # dataset_true = (dataset_true*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels]\n",
    "    # dataset_pred = (dataset_pred*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels]\n",
    "#     print(\"std\")\n",
    "#     print(dataset[\"train_data_stds\"])\n",
    "    print(dataset[\"train_data_stds\"][output_channels])\n",
    "    \n",
    "    total_loss/=(nodes)\n",
    "    print(few_shot_years, \"MONTHS\" , \"RMSE\", np.sqrt(total_loss))\n",
    "    \n",
    "    np.save(os.path.join(RESULT_DIR, \"{}_{}_{}\".format(file, index, \"true_{}\".format(fold))), dataset_true)\n",
    "    np.save(os.path.join(RESULT_DIR, \"{}_{}_{}_{}\".format(file, index, few_shot_years, model_name)), dataset_pred)\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Time:{:.4f}\".format(end-start))\n",
    "#         inverse_model.eval()\n",
    "#         forward_model.eval()\n",
    "#         _,years_test,_,_ = data_test.shape\n",
    "#         # for year in range(years_test):\n",
    "\n",
    "#         #Get instance for each node\n",
    "#         node_data_test = data_test[basin, :]\n",
    "# #         print(\"before \", dataset_code.shape)\n",
    "# # #         dataset_code = np.repeat(dataset_code, node_data_test.shape, axis=0)\n",
    "# #         print(\"after \", dataset_code.shape)\n",
    "# #         print(node_data_test.shape)\n",
    "\n",
    "#         # GET BATCH DATA AND LABEL\n",
    "#         batch_data = torch.from_numpy(node_data_test).unsqueeze(0).to(device)\n",
    "#         batch_label = batch_data[:, :,:,output_channels].to(device)\n",
    "        \n",
    "        \n",
    "#         batch_data = torch.from_numpy(node_data_test).unsqueeze(0).to(device)\n",
    "#         batch_dynamic_input = batch_data[:, :,:,dynamic_channels].to(device)\n",
    "# #         batch_static_input = batch_data[:, :,:,static_channels].to(device)\n",
    "# #         batch_label = batch_data[:, :,:,output_channels].to(device)\n",
    "#         # print(batch_input.shape, batch_label.shape)\n",
    "\n",
    "#         # GET INVERSE OUTPUT\n",
    "# #         print(\"batch_code_vec\",dataset_code[basin].shape\n",
    "#         batch_static_input = torch.from_numpy(dataset_code_repeat[basin]).to(device).unsqueeze(0).unsqueeze(1).repeat(1,1, window, 1)\n",
    "#         batch_static_input = batch_static_input.repeat(1,years_test, 1, 1).squeeze()\n",
    "\n",
    "#         # GET OUTPUT\n",
    "#         batch_pred = forward_model(x_dynamic=batch_dynamic_input.squeeze().float(), x_static=batch_static_input.float())\n",
    "#         data_test_original = data[:,48:]\n",
    "#         data_shape = data_test_original.shape\n",
    "#         original_years = data_shape[1]\n",
    "#         original_channels = data_shape[2]\n",
    "        \n",
    "#         #HERE I AM CHANGING THE DATA TEST\n",
    "#         data_test = np.reshape(data_test_original, (1, data_shape[0]*data_shape[1], data_shape[2], data_shape[3]))\n",
    "        \n",
    "#         _,years_test,_,_ = data_test.shape\n",
    "# #         print(data_test.shape)\n",
    "#         node_data_test = data_test[0, :]\n",
    "        \n",
    "#         batch_data = torch.from_numpy(node_data_test).unsqueeze(0).to(device)\n",
    "#         batch_dynamic_input = batch_data[:, :,:,dynamic_channels].to(device)\n",
    "# #         batch_static_input = batch_data[:, :,:,static_channels].to(device)\n",
    "#         batch_label = batch_data[:, :,:,output_channels].to(device)\n",
    "#         # print(batch_input.shape, batch_label.shape)\n",
    "\n",
    "#         # GET INVERSE OUTPUT\n",
    "# #         print(\"batch_code_vec\",dataset_code[basin].shape\n",
    "#         batch_static_input = torch.from_numpy(dataset_code_repeat[basin]).to(device).unsqueeze(0).unsqueeze(1).repeat(1,1, window, 1)\n",
    "#         batch_static_input = batch_static_input.repeat(1,years_test, 1, 1).squeeze()\n",
    "\n",
    "#         # GET OUTPUT\n",
    "        \n",
    "#         batch_pred = forward_model(x_dynamic=batch_dynamic_input.squeeze().float(), x_static=batch_static_input.float())\n",
    "# #         batch_pred_shape = batch_pred.shape\n",
    "# #         print(batch_pred.shape)\n",
    "# #         batch_pred = batch_pred.reshape((original_years, original_channels, 1))\n",
    "        \n",
    "#         # print(test_criterion(batch_label,batch_pred).item())\n",
    "#         total_loss+= test_criterion(batch_label,batch_pred).item()\n",
    "\n",
    "\n",
    "#         # STORE OUTPUT\n",
    "#         dataset_true[0, :,:,:] = batch_label.detach().cpu().numpy()\n",
    "#         dataset_pred[0, :,:,:] = batch_pred.detach().cpu().numpy()\n",
    "\n",
    "#     # dataset_true = (dataset_true*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels]\n",
    "#     # dataset_pred = (dataset_pred*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels]\n",
    "# #     print(dataset[\"train_data_stds\"])\n",
    "#     print(dataset[\"train_data_stds\"][output_channels])\n",
    "    \n",
    "#     total_loss/=(nodes)\n",
    "#     print(few_shot_years, \"MONTHS\" , \"RMSE\", np.sqrt(total_loss))\n",
    "    \n",
    "#     np.save(os.path.join(RESULT_DIR, \"{}_{}_{}\".format(file, index, \"true_{}\".format(fold))), dataset_true)\n",
    "#     np.save(os.path.join(RESULT_DIR, \"{}_{}_{}_{}\".format(file, index, few_shot_years, model_name)), dataset_pred)\n",
    "\n",
    "\n",
    "#     end = time.time()\n",
    "#     print(\"Time:{:.4f}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-native",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.sqrt(test_criterion(torch.tensor(dataset_true),torch.tensor(dataset_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-halifax",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = criterion(torch.tensor(dataset_true),torch.tensor(dataset_pred))\n",
    "\n",
    "\n",
    "print(test_loss.shape)\n",
    "print(torch.sqrt((torch.mean(test_loss, dim=(1,2,3)))))\n",
    "\n",
    "\n",
    "print(\"mean\",torch.mean(torch.sqrt((torch.mean(test_loss, dim=(1,2,3))))))\n",
    "print(\"median\",torch.median(torch.sqrt((torch.mean(test_loss, dim=(1,2,3))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "meaningful-magazine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape  (15, 1, 30, 24)\n",
      "ae(\n",
      "  (instance_encoder): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (temporal_encoder): LSTM(64, 64, batch_first=True, bidirectional=True)\n",
      "  (code_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (decode_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (temporal_decoder): LSTM(64, 64, batch_first=True)\n",
      "  (instance_decoder): Linear(in_features=64, out_features=6, bias=True)\n",
      "  (static_out): Linear(in_features=64, out_features=5, bias=True)\n",
      ")\n",
      "tamlstm(\n",
      "  (encoder): TAMLSTM(\n",
      "    (_embeddings): ModuleList(\n",
      "      (0): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (1): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (2): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (3): Linear(in_features=64, out_features=700, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (out): Linear(in_features=350, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (linear): Linear(in_features=350, out_features=350, bias=True)\n",
      ")\n",
      "#Parameters:914452\n",
      "dataset_code (1, 64)\n",
      "Time:0.2997\n",
      "train data shape  (15, 5, 30, 24)\n",
      "ae(\n",
      "  (instance_encoder): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (temporal_encoder): LSTM(64, 64, batch_first=True, bidirectional=True)\n",
      "  (code_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (decode_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (temporal_decoder): LSTM(64, 64, batch_first=True)\n",
      "  (instance_decoder): Linear(in_features=64, out_features=6, bias=True)\n",
      "  (static_out): Linear(in_features=64, out_features=5, bias=True)\n",
      ")\n",
      "tamlstm(\n",
      "  (encoder): TAMLSTM(\n",
      "    (_embeddings): ModuleList(\n",
      "      (0): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (1): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (2): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (3): Linear(in_features=64, out_features=700, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (out): Linear(in_features=350, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (linear): Linear(in_features=350, out_features=350, bias=True)\n",
      ")\n",
      "#Parameters:914452\n",
      "dataset_code (1, 64)\n",
      "Time:0.9694\n",
      "train data shape  (15, 11, 30, 24)\n",
      "ae(\n",
      "  (instance_encoder): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (temporal_encoder): LSTM(64, 64, batch_first=True, bidirectional=True)\n",
      "  (code_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (decode_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (temporal_decoder): LSTM(64, 64, batch_first=True)\n",
      "  (instance_decoder): Linear(in_features=64, out_features=6, bias=True)\n",
      "  (static_out): Linear(in_features=64, out_features=5, bias=True)\n",
      ")\n",
      "tamlstm(\n",
      "  (encoder): TAMLSTM(\n",
      "    (_embeddings): ModuleList(\n",
      "      (0): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (1): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (2): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (3): Linear(in_features=64, out_features=700, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (out): Linear(in_features=350, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (linear): Linear(in_features=350, out_features=350, bias=True)\n",
      ")\n",
      "#Parameters:914452\n",
      "dataset_code (1, 64)\n",
      "Time:2.0408\n",
      "train data shape  (15, 23, 30, 24)\n",
      "ae(\n",
      "  (instance_encoder): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (temporal_encoder): LSTM(64, 64, batch_first=True, bidirectional=True)\n",
      "  (code_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (decode_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (temporal_decoder): LSTM(64, 64, batch_first=True)\n",
      "  (instance_decoder): Linear(in_features=64, out_features=6, bias=True)\n",
      "  (static_out): Linear(in_features=64, out_features=5, bias=True)\n",
      ")\n",
      "tamlstm(\n",
      "  (encoder): TAMLSTM(\n",
      "    (_embeddings): ModuleList(\n",
      "      (0): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (1): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (2): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (3): Linear(in_features=64, out_features=700, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (out): Linear(in_features=350, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (linear): Linear(in_features=350, out_features=350, bias=True)\n",
      ")\n",
      "#Parameters:914452\n",
      "dataset_code (1, 64)\n",
      "Time:4.2307\n",
      "train data shape  (15, 47, 30, 24)\n",
      "ae(\n",
      "  (instance_encoder): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (temporal_encoder): LSTM(64, 64, batch_first=True, bidirectional=True)\n",
      "  (code_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (decode_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (temporal_decoder): LSTM(64, 64, batch_first=True)\n",
      "  (instance_decoder): Linear(in_features=64, out_features=6, bias=True)\n",
      "  (static_out): Linear(in_features=64, out_features=5, bias=True)\n",
      ")\n",
      "tamlstm(\n",
      "  (encoder): TAMLSTM(\n",
      "    (_embeddings): ModuleList(\n",
      "      (0): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (1): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (2): Linear(in_features=64, out_features=700, bias=True)\n",
      "      (3): Linear(in_features=64, out_features=700, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (out): Linear(in_features=350, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (linear): Linear(in_features=350, out_features=350, bias=True)\n",
      ")\n",
      "#Parameters:914452\n",
      "dataset_code (1, 64)\n",
      "Time:8.4787\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "all_few_shot_years = [1,3,6,12,24]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_inner_steps = 0\n",
    "\n",
    "for few_shot_years in all_few_shot_years:\n",
    "    start = time.time()\n",
    "\n",
    "\n",
    "    # num_years = 9\n",
    "\n",
    "\n",
    "\n",
    "    #     file, index = \"strided_test\", \"in_indices\"\n",
    "    #     dataset = load_dataset(file)\n",
    "    #     data_test = get_data(dataset, index,fold=fold)\n",
    "    #     nodes, years, window, channels = data_test.shape\n",
    "    file, index = \"strided_test\" ,\"out_indices\"\n",
    "    dataset = load_dataset(file)\n",
    "    data = get_data(dataset)  #[15,72,30,6]\n",
    "#     data = data.reshape((1, years * nodes, window, len(output_channels)))\n",
    "    data_test = data[:,48:]\n",
    "#     data = data.reshape((1, years * nodes, window, len(output_channels)))\n",
    "\n",
    "    nodes, years, window, channels = data_test.shape     #TAKE A LOOK HERE\n",
    "    dataset_true = unknown*np.ones((nodes, years, window, len(output_channels)), dtype=np.float32)\n",
    "#     dataset_true = unknown*np.ones((1, years * nodes, window, len(output_channels)), dtype=np.float32)\n",
    "    dataset_pred = unknown*np.ones((nodes, years, window, len(output_channels)), dtype=np.float32)\n",
    "#     dataset_pred = unknown*np.ones((1, years * nodes, window, len(output_channels)), dtype=np.float32)\n",
    "\n",
    "    train_data = data[:,-((few_shot_years*2)-1):]  #HERE\n",
    "    print(\"train data shape \", train_data.shape)\n",
    "    nodes, years, window, channels = train_data.shape  \n",
    "    train_data = train_data.reshape((1, years * nodes, window, -1))\n",
    "    \n",
    "    nodes, years, window, channels = train_data.shape    #TAKE A LOOK HERE\n",
    "#     print(train_data.shape)\n",
    "\n",
    "    # train_data = train_data[:,:(num_years*2-1),:,:]\n",
    "    # train_data = train_data[:,years-num_years*2+1:,:,:]\n",
    "    nodes, years, window, channels = train_data.shape     #TAKE A LOOK HERE\n",
    "#     print(train_data.shape)\n",
    "    \n",
    "    file, index = \"strided_test\", \"out_indices\"\n",
    "\n",
    "\n",
    "    # BUILD MODEL\n",
    "    inverse_model = getattr(MODEL, \"ae\")(input_channels=len(dynamic_channels)+len(output_channels), code_dim=latent_code_dim, hidden_dim=latent_code_dim, output_channels=len(static_channels), device=device)\n",
    "    inverse_model = inverse_model.to(device)\n",
    "    pytorch_total_params = sum(p.numel() for p in inverse_model.parameters() if p.requires_grad)\n",
    "    print(inverse_model)\n",
    "    forward_model = getattr(MODEL, \"tamlstm\")(input_dynamic_channels=len(dynamic_channels), input_static_channels=latent_code_dim, hidden_dim=forward_code_dim, output_channels=len(output_channels), dropout=dropout)\n",
    "    forward_model = forward_model.to(device)\n",
    "    pytorch_total_params += sum(p.numel() for p in forward_model.parameters() if p.requires_grad)\n",
    "    print(forward_model)\n",
    "    print(\"#Parameters:{}\".format(pytorch_total_params))\n",
    "    criterion = torch.nn.MSELoss(reduction=\"none\")\n",
    "    test_criterion = torch.nn.MSELoss(reduction=\"mean\")\n",
    "    optimizer_embedding = torch.optim.Adam(list(inverse_model.parameters()), lr=learning_rate)\n",
    "    optimizer_forward = torch.optim.Adam(list(forward_model.parameters()), lr=learning_rate)\n",
    "    # print(\"#Parameters:{}\".format(pytorch_total_params))\n",
    "    # print(model)\n",
    "\n",
    "    # LOAD MODEL\n",
    "    inverse_model.load_state_dict(torch.load(os.path.join(MODEL_DIR, \"{}_inverse\".format(model_name))))\n",
    "    inverse_model.eval()\n",
    "    forward_model.load_state_dict(torch.load(os.path.join(MODEL_DIR, \"{}_forward\".format(model_name))))\n",
    "    dataset_code = unknown*np.ones((nodes, years, latent_code_dim), dtype=np.float32)\n",
    "\n",
    "\n",
    "    for year in range(years):\n",
    "\n",
    "        #Get instances for each node\n",
    "        node_data = train_data[np.arange(nodes), year]\n",
    "        # print(node_data.shape)\n",
    "\n",
    "        # Remove pairs where node have unknown in streamflow\n",
    "        keep_idx = np.zeros((node_data.shape[0], 1)).astype(bool)\n",
    "        keep_idx[:,0] = (node_data[:,:,-1]!=unknown).all(axis=1)\n",
    "        keep_idx = keep_idx.all(axis=1)\n",
    "        node_data = node_data[keep_idx]\n",
    "        # print(node_data.shape)\n",
    "\n",
    "        for batch in range(math.ceil(node_data.shape[0]/batch_size)):\n",
    "\n",
    "            # GET BATCH DATA FOR INVERSE MODEL\n",
    "            batch_data = torch.from_numpy(node_data[batch*batch_size:(batch+1)*batch_size]).to(device)\n",
    "            batch_input = batch_data[:,:,dynamic_channels+output_channels]\n",
    "            batch_static = batch_data[:,0,static_channels]\n",
    "            # print(batch_input.shape, batch_static.shape)\n",
    "\n",
    "            # GET INVERSE OUTPUT\n",
    "            batch_code_vec, batch_static_pred, batch_input_pred,_ = inverse_model(x=batch_input.float())\n",
    "            # print(batch_code_vec.shape, batch_static_pred.shape, batch_input_pred.shape)\n",
    "\n",
    "            # STORE OUTPUT\n",
    "            dataset_code[batch*batch_size:(batch+1)*batch_size, year] = batch_code_vec.detach().cpu().numpy()\n",
    "    dataset_code = np.mean(dataset_code, axis=1)\n",
    "    print(\"dataset_code\",dataset_code.shape)\n",
    "    \n",
    "    \n",
    "    dataset_code = np.repeat(dataset_code, data_test.shape[0], axis=0)\n",
    "    \n",
    "    np.save(os.path.join(RESULT_DIR, \"{}_{}_{}_{}_Embedding\".format(file, index, few_shot_years, model_name)), dataset_code)\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Time:{:.4f}\".format(end-start))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-calibration",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_a100",
   "language": "python",
   "name": "main_a100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
